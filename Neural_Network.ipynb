{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae4893de-7f6a-48b8-81c1-5a0d9e8e1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d71a0-f880-406e-85b4-380e8b7d67aa",
   "metadata": {},
   "source": [
    "## Initialize Parameters for L Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "543ae160-871e-4da5-9dcd-db4739e636a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dim, seed = 3):\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dim: Array with the dimensions of each layer of the network\n",
    "    seed: random seed\n",
    "    \n",
    "    Return:\n",
    "    W: weight matrix (n_l, n_l-1)\n",
    "    b: bias vector (n_l, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    L = len(layer_dim) # Layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dim[l], layer_dim[l-1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dim[l], 1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbbcc1-d3b6-49e4-ac1c-5b6caa3ec8dc",
   "metadata": {},
   "source": [
    "**This is an example for Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f4fa6e-e478-48aa-b208-2160c1e054fd",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"Images/1.png\" width=\"600\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc1c46c-a5c0-4f60-9baf-85e7cdcfc894",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e645aa-0d47-4bdf-a1a2-570b9439c3e0",
   "metadata": {},
   "source": [
    "<img src=\"Images/2.png\" width=\"600\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a69e8-2f1f-4fb9-85cc-50cb5fd29341",
   "metadata": {},
   "source": [
    "<img src=\"Images/3.png\" width=\"600\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8abf5159-30f4-476e-8f85-a616bb2bae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    A: Values form the previous layer (A0 is the input layer with X values)\n",
    "    W: weight matrix\n",
    "    b: bias vector\n",
    "    \n",
    "    Return\n",
    "    Z: Preactivaction parameter\n",
    "    cache: a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cc890e3-7f87-4f34-9bef-43adf1b1a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    A_prev: activations from previous layer (or input data)\n",
    "    W: weights matrix\n",
    "    b: bias vector\n",
    "    activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A: the output of the activation function\n",
    "    cache: a python tuple containing \"linear_cache\" and \"activation_cache\" stored for computing the backward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = (1/(1+np.exp(-Z)), Z)\n",
    "        \n",
    "    elif activation == \"relu\":                      \n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = (np.maximum(0, Z), Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b439549-28ba-4d5c-a06c-590d8da40569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    X: data\n",
    "    parameters: output of initialize_parameters()\n",
    "    \n",
    "    Returns:\n",
    "    AL: activation value from the output (last) layer\n",
    "    caches: list of caches containing: every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # The for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        W = parameters[f\"W{l}\"]\n",
    "        b = parameters[f\"b{l}\"]\n",
    "        A, cache = linear_activation_forward(A_prev, W, b, activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    W = parameters[f\"W{L}\"]\n",
    "    b = parameters[f\"b{L}\"]\n",
    "    AL, cache = linear_activation_forward(A, W, b, activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "  \n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee56b09-8f4f-403e-a71d-958908ed02a2",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ae756-bc26-4303-9e7c-62212946aec1",
   "metadata": {},
   "source": [
    "<img src=\"Images/4.png\" width=\"600\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f46d588-a543-49b2-8421-1910811dab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions\n",
    "    Y -- true \"label\" vector \n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -1/m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "         \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3614f4e5-a22b-4907-84e3-a6040daa4181",
   "metadata": {},
   "source": [
    "## Backwward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b7e95-93b8-4ba4-aea2-8e97c8be12dd",
   "metadata": {},
   "source": [
    "<img src=\"Images/5.png\" width=\"600\">  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4be9a24-e768-42f4-a5ed-1d19893acd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4a9f4af-29dd-459f-9cfa-44145958f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    Z = activation_cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ = dZ[Z <= 0] = 0\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "  \n",
    "    elif activation == \"sigmoid\":\n",
    "        s = 1 / (1 + np.exp(-Z))\n",
    "        dZ = dA * s * (1 - s)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae0889c8-37a7-46eb-b028-bf78234ae5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L - 1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    grads[\"dA\" + str(L - 1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "     \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33205fc6-799c-40d3-b0a3-776950d740e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ece9091f-640a-4fe2-a1d9-206eb68221ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_path = 'datasets/train_catvsnoncat.h5'\n",
    "test_path = 'datasets/test_catvsnoncat.h5'\n",
    "train = h5py.File(train_path, \"r\")\n",
    "test = h5py.File(test_path, \"r\")\n",
    "\n",
    "# Extract data\n",
    "X_train = np.array(train[\"train_set_x\"][:])  \n",
    "y_train = np.array(train[\"train_set_y\"][:])  \n",
    "X_test = np.array(test[\"test_set_x\"][:])     \n",
    "y_test = np.array(test[\"test_set_y\"][:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f0114b9-4be5-4037-8ea5-f07e929c8633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 64, 64, 3) (209,)\n",
      "(50, 64, 64, 3) (50,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad53e0c4-232e-4849-b6f3-187540490a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_flat: (209, 12288)\n",
      "y_train: (209, 1)\n",
      "X_test_flat: (50, 12288)\n",
      "y_test: (50, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape image data to (num_samples, 64*64*3)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "# Reshape labels to column vector (num_samples, 1)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(\"X_train_flat:\", X_train_flat.shape)   # Should be (209, 12288)\n",
    "print(\"y_train:\", y_train.shape)             # Should be (209, 1)\n",
    "\n",
    "print(\"X_test_flat:\", X_test_flat.shape)     # Should be (50, 12288)\n",
    "print(\"y_test:\", y_test.shape)               # Should be (50, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
